---
title: 'Assignment3_AY'
author: "AY Kuwornu"
date: "2024-11-05"
output: html_document
---

#Load the appropriate software libraries
```{r}
#1.install.packages("pdftools")
library(tidyverse)
library(pdftools)
```

#Import the data and compile the articles into a dataframe, one row per sentence.
```{r}
#Using pdftools package. Good for basic PDF extraction
#Nov 5: removed split_file folder in a cleanup
text <- pdf_text("../Assignment3_AYKuwornu/moley_news.PDF")
#pdf_text reads the text from a PDF file.
writeLines(text, "../Assignment3_AYKuwornu/moley_news.txt")
#writeLines writes this text to a text file

## Split text to separate articles on common identifier
# Step 1: Read text file into R
file_path <- "../Assignment3_AYKuwornu/moley_news.txt"
text_data <- readLines(file_path)

# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]

# Step 4: Write each section to a new file
output_dir <- "../Assignment3_AYKuwornu/Extracted/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moley_extracted_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")
```
```{r}
## Create an index from the first extracted page
ay_moley_index <- read_lines("../Assignment3_AYKuwornu/Extracted/moley_extracted_1.txt")
# Extract lines 16 to 91
extracted_lines <- ay_moley_index[6:176]

# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 

```
## Build a final dataframe index

```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


#Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "Extracted/final_data.csv")
  
```

# Part 2: Compile Text into a Dataframe

## Raw text compiler 
```{r}

files <- list.files("../Assignment3_AYKuwornu/Extracted", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

#the actual path: #C:/Users/elikp/OneDrive/Desktop/My_R_Class/CompText_Jour/Assignment3_AYKuwornu/Extracted

#Join the file list to the index

#load final data if you haven't already
#final_data <- read.csv("C:/Users/elikp/OneDrive/Desktop/My_R_Class/CompText_Jour/Assignment3_AYKuwornu/Extracted/final_data.csv")

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  
  # mutate(filepath = paste0("../Spooky/Extracted", filename))
  mutate(filepath = paste0("C:/Users/elikp/OneDrive/Desktop/My_R_Class/CompText_Jour/Assignment3_AYKuwornu/Extracted/", filename))
head(final_index)
```

## Text compiler
```{r}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 158 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:158)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

#write.csv(articles_df, "../Assignment3_AYKuwornu/Extracted/ay_moley.csv)
write.csv(articles_df, "../Assignment3_AYKuwornu/Extracted/ay_moley.csv")

```

#Then tokenize the data, one word per row, Clean the data and Generate a list of the top 20 bigrams
```{r}
#Step 1:load tidyverse, tidytext, rio 
library(tidyverse)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
library(dplyr)

#Step 2: Remove stopwords
data(stop_words)


#Step 3: Clean data and create Bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_squish(sentence)) |> 
  mutate(sentence = tolower(sentence)) |>  
  mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier|Privacy Policy|title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|Author|keyword|twitter\\.", "")) |> 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigrams


#Step 4: Generate top 20 bigrams
top_20_bigrams <- bigrams %>% 
   slice_max(n, n = 20) %>% 
   mutate(bigram = paste(word1, " ", word2)) %>% 
   select(bigram, n)

top_20_bigrams
```

```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top 20 bigrams in Moley article",
       caption = "n=36 articles. Graphic by AY Kuwornu. 11-05-2024",
       x = "Phrase",
       y = "Count of terms")
```

The thirty-two articles about Raymond Moley was easy to scrape using the pdftools package. The codes worked fairly well for me, especially as I applied them procedurally. My biggest challenge was creating the short path, which I did through the files section of the R markdown file to the bottom right corner of the screen. However, when I tried downloading the R-markdown file on my colleagues computer to run the code, I run into a snug. I alternated by using the github site address to the file and it still hit a snug when I run it on a colleague's file. In the absence of that, the code chunk worked perfectly for me.

My second finding was during the cleaning of the data. As hard as we tried, listing all the assumed metadata didn't do much to the data. Some metadata cleaned, but some were very difficult to clean. We tried varied ways of getting rid of "keyword" for example, but noticed that there were still some keywords in the final data. It's hard to understand what we did not do right, because we used the str_replace_all command which ideally should remove all the metadata. That still failed.

My third finding reflected in the bigrams which had some elements that were offshoots from the failed metadata cleaning. For example, wwwalt morg was an incomprehensible bigram which we suspected was a biproduct of the failure of our command to clean anything with "www". That said, we were not sure to what extent our assumptions were right. Nonetheless, our graph showed a nice collection of bigrams, the top 20 which reflected some key bigrams, e.g. economic recovery, columbia university, rights reserved, etc.

This exercise showed the comprehensive powers of R to help with slicing and dicing data to arrive at meaningful interpretations. Once I am able to cross the file path difficulty, I will be able to do very amazing things with this tool.